{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057a892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk3/raman/code/anaconda3/envs/raman/lib/python3.11/site-packages/oauth2client/_helpers.py:255: UserWarning: Cannot access credentials.json: No such file or directory\n",
      "  warnings.warn(_MISSING_FILE_MESSAGE.format(filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=903213563690-dc8onmle8ebf0d8qj557hl6ot54snj8q.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n",
      "Downloading tile_223.csv\n",
      "Downloading tile_238.csv\n",
      "Downloading tile_239.csv\n",
      "Downloading tile_237.csv\n",
      "Downloading tile_236.csv\n",
      "Downloading tile_228.csv\n",
      "Downloading tile_235.csv\n",
      "Downloading tile_234.csv\n",
      "Downloading tile_233.csv\n",
      "Downloading tile_232.csv\n",
      "Downloading tile_231.csv\n",
      "Downloading tile_230.csv\n",
      "Downloading tile_229.csv\n",
      "Downloading tile_227.csv\n",
      "Downloading tile_226.csv\n",
      "Downloading tile_225.csv\n",
      "Downloading tile_187.csv\n",
      "Downloading tile_224.csv\n",
      "Downloading tile_222.csv\n",
      "Downloading tile_220.csv\n",
      "Downloading tile_221.csv\n",
      "Downloading tile_219.csv\n",
      "Downloading tile_218.csv\n",
      "Downloading tile_217.csv\n",
      "Downloading tile_214.csv\n",
      "Downloading tile_216.csv\n",
      "Downloading tile_215.csv\n",
      "Downloading tile_213.csv\n",
      "Downloading tile_212.csv\n",
      "Downloading tile_211.csv\n",
      "Downloading tile_210.csv\n",
      "Downloading tile_208.csv\n",
      "Downloading tile_209.csv\n",
      "Downloading tile_205.csv\n",
      "Downloading tile_207.csv\n",
      "Downloading tile_206.csv\n",
      "Downloading tile_203.csv\n",
      "Downloading tile_204.csv\n",
      "Downloading tile_201.csv\n",
      "Downloading tile_202.csv\n",
      "Downloading tile_198.csv\n",
      "Downloading tile_200.csv\n",
      "Downloading tile_199.csv\n",
      "Downloading tile_197.csv\n",
      "Downloading tile_196.csv\n",
      "Downloading tile_195.csv\n",
      "Downloading tile_194.csv\n",
      "Downloading tile_191.csv\n",
      "Downloading tile_193.csv\n",
      "Downloading tile_192.csv\n",
      "Downloading tile_190.csv\n",
      "Downloading tile_188.csv\n",
      "Downloading tile_189.csv\n",
      "Downloading tile_186.csv\n",
      "Downloading tile_183.csv\n",
      "Downloading tile_185.csv\n",
      "Downloading tile_184.csv\n",
      "Downloading tile_182.csv\n",
      "Downloading tile_178.csv\n",
      "Downloading tile_181.csv\n",
      "Downloading tile_180.csv\n",
      "Downloading tile_179.csv\n",
      "Downloading tile_177.csv\n",
      "Downloading tile_176.csv\n",
      "Downloading tile_175.csv\n",
      "Downloading tile_174.csv\n",
      "Downloading tile_173.csv\n",
      "Downloading tile_172.csv\n",
      "Downloading tile_171.csv\n",
      "Downloading tile_169.csv\n",
      "Downloading tile_170.csv\n",
      "Downloading tile_168.csv\n",
      "Downloading tile_167.csv\n",
      "Downloading tile_165.csv\n",
      "Downloading tile_161.csv\n",
      "Downloading tile_166.csv\n",
      "Downloading tile_163.csv\n",
      "Downloading tile_164.csv\n",
      "Downloading tile_162.csv\n",
      "Downloading tile_160.csv\n",
      "Downloading tile_159.csv\n",
      "Downloading tile_155.csv\n",
      "Downloading tile_158.csv\n",
      "Downloading tile_157.csv\n",
      "Downloading tile_152.csv\n",
      "Downloading tile_156.csv\n",
      "Downloading tile_154.csv\n",
      "Downloading tile_153.csv\n",
      "Downloading tile_151.csv\n",
      "Downloading tile_143.csv\n",
      "Downloading tile_148.csv\n",
      "Downloading tile_150.csv\n",
      "Downloading tile_149.csv\n",
      "Downloading tile_147.csv\n",
      "Downloading tile_146.csv\n",
      "Downloading tile_140.csv\n",
      "Downloading tile_145.csv\n",
      "Downloading tile_144.csv\n",
      "Downloading tile_139.csv\n",
      "Downloading tile_142.csv\n",
      "Downloading tile_141.csv\n",
      "Downloading tile_138.csv\n",
      "Downloading tile_134.csv\n",
      "Downloading tile_137.csv\n",
      "Downloading tile_136.csv\n",
      "Downloading tile_133.csv\n",
      "Downloading tile_135.csv\n",
      "Downloading tile_132.csv\n",
      "Downloading tile_130.csv\n",
      "Downloading tile_131.csv\n",
      "Downloading tile_129.csv\n",
      "Downloading tile_128.csv\n",
      "Downloading tile_126.csv\n",
      "Downloading tile_127.csv\n",
      "Downloading tile_125.csv\n",
      "Downloading tile_124.csv\n",
      "Downloading tile_123.csv\n",
      "Downloading tile_122.csv\n",
      "Downloading tile_121.csv\n",
      "Downloading tile_120.csv\n",
      "Downloading tile_118.csv\n",
      "Downloading tile_119.csv\n",
      "Downloading tile_112.csv\n",
      "Downloading tile_117.csv\n",
      "Downloading tile_116.csv\n",
      "Downloading tile_115.csv\n",
      "Downloading tile_114.csv\n",
      "Downloading tile_113.csv\n",
      "Downloading tile_110.csv\n",
      "Downloading tile_111.csv\n",
      "Downloading tile_109.csv\n",
      "Downloading tile_102.csv\n",
      "Downloading tile_107.csv\n",
      "Downloading tile_108.csv\n",
      "Downloading tile_106.csv\n",
      "Downloading tile_105.csv\n",
      "Downloading tile_104.csv\n",
      "Downloading tile_103.csv\n",
      "Downloading tile_100.csv\n",
      "Downloading tile_101.csv\n",
      "Downloading tile_99.csv\n",
      "Downloading tile_98.csv\n",
      "Downloading tile_96.csv\n",
      "Downloading tile_97.csv\n",
      "Downloading tile_95.csv\n",
      "Downloading tile_93.csv\n",
      "Downloading tile_94.csv\n",
      "Downloading tile_92.csv\n",
      "Downloading tile_91.csv\n",
      "Downloading tile_90.csv\n",
      "Downloading tile_88.csv\n",
      "Downloading tile_89.csv\n",
      "Downloading tile_86.csv\n",
      "Downloading tile_87.csv\n",
      "Downloading tile_84.csv\n",
      "Downloading tile_85.csv\n",
      "Downloading tile_82.csv\n",
      "Downloading tile_83.csv\n",
      "Downloading tile_81.csv\n",
      "Downloading tile_79.csv\n",
      "Downloading tile_80.csv\n",
      "Downloading tile_78.csv\n",
      "Downloading tile_76.csv\n",
      "Downloading tile_77.csv\n",
      "Downloading tile_75.csv\n",
      "Downloading tile_74.csv\n",
      "Downloading tile_73.csv\n",
      "Downloading tile_72.csv\n",
      "Downloading tile_70.csv\n",
      "Downloading tile_67.csv\n",
      "Downloading tile_71.csv\n",
      "Downloading tile_69.csv\n",
      "Downloading tile_68.csv\n",
      "Downloading tile_64.csv\n",
      "Downloading tile_66.csv\n",
      "Downloading tile_65.csv\n",
      "Downloading tile_60.csv\n",
      "Downloading tile_63.csv\n",
      "Downloading tile_62.csv\n",
      "Downloading tile_61.csv\n",
      "Downloading tile_58.csv\n",
      "Downloading tile_59.csv\n",
      "Downloading tile_53.csv\n",
      "Downloading tile_56.csv\n",
      "Downloading tile_57.csv\n",
      "Downloading tile_55.csv\n",
      "Downloading tile_54.csv\n",
      "Downloading tile_51.csv\n",
      "Downloading tile_52.csv\n",
      "Downloading tile_50.csv\n",
      "Downloading tile_48.csv\n",
      "Downloading tile_49.csv\n",
      "Downloading tile_47.csv\n",
      "Downloading tile_46.csv\n",
      "Downloading tile_45.csv\n",
      "Downloading tile_44.csv\n",
      "Downloading tile_43.csv\n",
      "Downloading tile_42.csv\n",
      "Downloading tile_41.csv\n",
      "Downloading tile_32.csv\n",
      "Downloading tile_40.csv\n",
      "Downloading tile_39.csv\n",
      "Downloading tile_38.csv\n",
      "Downloading tile_37.csv\n",
      "Downloading tile_36.csv\n",
      "Downloading tile_35.csv\n",
      "Downloading tile_34.csv\n",
      "Downloading tile_33.csv\n",
      "Downloading tile_30.csv\n",
      "Downloading tile_31.csv\n",
      "Downloading tile_29.csv\n",
      "Downloading tile_27.csv\n",
      "Downloading tile_28.csv\n",
      "Downloading tile_26.csv\n",
      "Downloading tile_25.csv\n",
      "Downloading tile_23.csv\n",
      "Downloading tile_24.csv\n",
      "Downloading tile_22.csv\n",
      "Downloading tile_20.csv\n",
      "Downloading tile_21.csv\n",
      "Downloading tile_18.csv\n",
      "Downloading tile_19.csv\n",
      "Downloading tile_17.csv\n",
      "Downloading tile_16.csv\n",
      "Downloading tile_15.csv\n",
      "Downloading tile_14.csv\n",
      "Downloading tile_13.csv\n",
      "Downloading tile_12.csv\n",
      "Downloading tile_10.csv\n",
      "Downloading tile_11.csv\n",
      "Downloading tile_8.csv\n",
      "Downloading tile_9.csv\n",
      "Downloading tile_7.csv\n",
      "Downloading tile_6.csv\n",
      "Downloading tile_5.csv\n",
      "Downloading tile_4.csv\n",
      "Downloading tile_3.csv\n",
      "Downloading tile_2.csv\n",
      "Downloading tile_1.csv\n",
      "Downloading tile_0.csv\n"
     ]
    }
   ],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import os\n",
    "\n",
    "AEZ_no = 6\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "# Ensure you have client_secrets.json in the working directory.\n",
    "gauth.LoadClientConfigFile(\"client_secrets_1.json\")\n",
    "\n",
    "gauth.LoadCredentialsFile(\"credentials.json\")\n",
    "\n",
    "if gauth.credentials is None:\n",
    "    # First run: open browser, ask once\n",
    "    gauth.CommandLineAuth()\n",
    "elif gauth.access_token_expired:\n",
    "    # Auto-refresh using the refresh token\n",
    "    gauth.Refresh()\n",
    "else:\n",
    "    gauth.Authorize()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Folder name in Google Drive\n",
    "folder_name = 'Scrubland_Field_Delineation/AEZ_'+str(AEZ_no)\n",
    "local_folder = '../AEZ_'+str(AEZ_no)+\"/Samples\"\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "# Get folder ID\n",
    "file_list = drive.ListFile({'q': \"mimeType='application/vnd.google-apps.folder' and trashed=false\"}).GetList()\n",
    "folder_id = None\n",
    "for file in file_list:\n",
    "    if file['title'] == folder_name:\n",
    "        folder_id = file['id']\n",
    "        break\n",
    "\n",
    "if folder_id is None:\n",
    "    print(\"Folder not found!\")\n",
    "else:\n",
    "    # List all CSV files in the folder\n",
    "    query = f\"'{folder_id}' in parents and trashed=false and mimeType='text/csv'\"\n",
    "    file_list = drive.ListFile({'q': query}).GetList()\n",
    "    for file in file_list:\n",
    "        print(f\"Downloading {file['title']}\")\n",
    "        file.GetContentFile(os.path.join(local_folder, file['title']))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456566ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob('../AEZ_'+str(AEZ_no)+'/Samples/tile_*.csv')\n",
    "combined_csv = '../AEZ_'+str(AEZ_no)+'/Samples/combined_samples.csv'\n",
    "df_list = []\n",
    "try:\n",
    "    for i in range(len(csv_files)):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_files[i])\n",
    "            if not df.empty:\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                print(f\"Skipping empty file: {csv_files[i]}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Skipping empty file: {csv_files[i]}\")\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        filtered_df = combined_df[combined_df['label'] != 0]\n",
    "        filtered_df.to_csv(combined_csv, index=False)\n",
    "except Exception as e:\n",
    "    print(i)\n",
    "    print(f\"Error processing file {csv_files[i]}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d75f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system:index</th>\n",
       "      <th>label</th>\n",
       "      <th>.geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>452</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>453</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>454</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191999</th>\n",
       "      <td>819</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192000</th>\n",
       "      <td>820</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192001</th>\n",
       "      <td>821</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192002</th>\n",
       "      <td>822</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192003</th>\n",
       "      <td>823</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84004 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        system:index  label                                               .geo\n",
       "450              450      1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "451              451      1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "452              452      1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "453              453      1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "454              454      1  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "...              ...    ...                                                ...\n",
       "191999           819      3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "192000           820      3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "192001           821      3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "192002           822      3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "192003           823      3  {\"geodesic\":false,\"type\":\"Point\",\"coordinates\"...\n",
       "\n",
       "[84004 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a56b170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_0\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_0 completed successfully.\n",
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_1\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_1 completed successfully.\n",
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_2\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_2 completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import time\n",
    "import json\n",
    "\n",
    "ee.Authenticate() \n",
    "ee.Initialize(project='raman-461708')\n",
    "\n",
    "def clean_geojson(geojson_str):\n",
    "    geo = json.loads(geojson_str)\n",
    "    # Remove 'geodesic' if present\n",
    "    geo.pop('geodesic', None)\n",
    "    return geo\n",
    "\n",
    "def df_to_fc(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse the geometry from the '.geo' column (assumed to be GeoJSON)\n",
    "        geom = ee.Geometry(clean_geojson(row['.geo']))\n",
    "        # Add all properties except '.geo'\n",
    "        props = row.drop('.geo').drop('system:index').to_dict()\n",
    "        features.append(ee.Feature(geom, props))\n",
    "    return ee.FeatureCollection(features)\n",
    "\n",
    "def write_to_gee(fc, asset_name):\n",
    "    \n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=fc,\n",
    "        description='Working on ' + asset_name.split('/')[-1],\n",
    "        assetId=asset_name,\n",
    "    )\n",
    "    task.start()\n",
    "    print(f\"Started export to GEE asset: {asset_name}\")\n",
    "    while True:\n",
    "        status = task.status()\n",
    "        state = status.get('state', 'UNKNOWN')\n",
    "        print(f\"Export status: {state}\")\n",
    "        if state in ['COMPLETED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "        time.sleep(30)\n",
    "    if state == 'COMPLETED':\n",
    "        print(f\"Export to GEE asset {asset_name} completed successfully.\")\n",
    "    else:\n",
    "        print(f\"Export to GEE asset {asset_name} failed with status: {state}\")\n",
    "\n",
    "chunk_size = 30000\n",
    "samples_paths = []\n",
    "for index, i in enumerate(range(0, len(filtered_df), chunk_size)):\n",
    "    chunk = filtered_df.iloc[i:i+chunk_size]\n",
    "    fc = df_to_fc(chunk)\n",
    "    asset_name = f'projects/raman-461708/assets/AEZ_{AEZ_no}_samples_{index}'\n",
    "    write_to_gee(fc, asset_name)\n",
    "    samples_paths.append(asset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c2e52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each asset path as a FeatureCollection and merge them\n",
    "samples = ee.FeatureCollection([])  # start with empty\n",
    "for asset_path in samples_paths:\n",
    "    fc = ee.FeatureCollection(asset_path)\n",
    "    samples = samples.merge(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27e7e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import ee\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import folium\n",
    "from folium import plugins\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57b70215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_collection(collec, ind):\n",
    "    \"\"\"\n",
    "    Sometimes directly getting ith feature form collection doesn't work\n",
    "    This hack works all the time. Converting collection to list and getting\n",
    "    ith element and re-casting it as feature.\n",
    "    \"\"\"\n",
    "    return ee.Feature(collec.toList(collec.size().getInfo()).get(ind))\n",
    "\n",
    "def displayMap(roi_boundary, image):\n",
    "  centroid = roi_boundary.geometry().centroid()\n",
    "  coordinates = centroid.coordinates()\n",
    "  centerLat = coordinates.get(1).getInfo()\n",
    "  centerLon = coordinates.get(0).getInfo()\n",
    "  mapObj = folium.Map(width='100%', height='80%', location=[centerLat, centerLon], zoom_start=50)\n",
    "\n",
    "  # Sattelite image visual parameters\n",
    "  vis_params = {\n",
    "    'min': 0,\n",
    "    'max': 12,\n",
    "    'palette': ['#000000',  # 0 Black- background\n",
    "              '#ff0000',   # 1 Red- builtup\n",
    "              '#74ccf4', # 2 Light Blue- kharif water\n",
    "              '#1ca3ec', # 3 Blue- kharif and rabi water\n",
    "              '#0f5e9c', # 4 Dark Blue- kharif and rabi and zaid water\n",
    "              '#f1c232', # 5 Yellow- croplands\n",
    "              '#38761d', # 6 Dark Green- Tree/Forests\n",
    "              '#A9A9A9', # 7 Gray- barren lands\n",
    "              '#f1c232', # 8 Yellow- Single Kharif Cropping\n",
    "              '#f59d22', # 9 Mustard- Single Non-Kharif Cropping\n",
    "              '#e68600', # 10 Orange- Double Cropping\n",
    "              '#b3561d', # 11 Brown- Triple Cropping\n",
    "              '#c39797' # 12 Mauve- Shrubs_Scrubs\n",
    "            ]\n",
    "    }\n",
    "\n",
    "  map_id_dict = ee.Image(image).getMapId(vis_params)\n",
    "\n",
    "  folium.TileLayer(\n",
    "          tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "          attr='Esri',\n",
    "          name='Esri Satellite',\n",
    "          overlay=True,\n",
    "          control=True\n",
    "      ).add_to(mapObj)\n",
    "\n",
    "  folium.raster_layers.TileLayer(\n",
    "                  tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "                  attr = 'Google Earth Engine',\n",
    "                  name = 'Sentinel 2 image',\n",
    "                  overlay = True,\n",
    "                  control = True\n",
    "                  ).add_to(mapObj)\n",
    "\n",
    "  mapObj.add_child(folium.LayerControl())\n",
    "  display(mapObj)\n",
    "\n",
    "\n",
    "'''\n",
    "Function to mask clouds based on the QA60 band of Sentinel SR data.\n",
    "param {ee.Image} image Input Sentinel SR image\n",
    "return {ee.Image} Cloudmasked Sentinel-2 image\n",
    "'''\n",
    "def maskS2cloud(image):\n",
    "  qa = image.select('QA60')\n",
    "  #Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "  cloudBitMask = 1 << 10\n",
    "  cirrusBitMask = 1 << 11\n",
    "  #Both flags should be set to zero, indicating clear conditions.\n",
    "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "  return image.updateMask(mask).divide(10000)\n",
    "\n",
    "'''\n",
    "Function to clean builtup predictions using NDWI.\n",
    "'''\n",
    "def ndwi_based_builtup_cleaning(roi_boundary, prediction_image, startDate, endDate, NDWI_threshold):\n",
    "  S2_ic = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \\\n",
    "            .map(maskS2cloud)\n",
    "\n",
    "  if S2_ic.size().getInfo() != 0:\n",
    "    S2_ic = S2_ic.map( lambda img: img.addBands(img.normalizedDifference(['B3', 'B8']).rename('NDWI')))\n",
    "    NDWI_max_img = S2_ic.select('NDWI').max().clip(roi_boundary.geometry())\n",
    "\n",
    "    corrected_water_img = prediction_image.select('predicted_label').where(prediction_image.select('predicted_label').neq(0).And(NDWI_max_img.gt(NDWI_threshold)), 0)\n",
    "    return corrected_water_img\n",
    "  else:\n",
    "    print(\"NDWI based builtup correction cannot be performed due to unavailability of Sentinel-2 data\")\n",
    "    return prediction_image\n",
    "\n",
    "\n",
    "'''\n",
    "Function to clean builtup predictions using NDVI.\n",
    "'''\n",
    "def ndvi_based_builtup_cleaning(roi_boundary, prediction_image, startDate, endDate, NDVI_threshold):\n",
    "  S2_ic = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \\\n",
    "            .map(maskS2cloud)\n",
    "\n",
    "  if S2_ic.size().getInfo() != 0:\n",
    "    S2_ic = S2_ic.map( lambda img: img.addBands(img.normalizedDifference(['B8', 'B4']).rename('NDVI')))\n",
    "    NDVI_max_img = S2_ic.select('NDVI').max().clip(roi_boundary.geometry())\n",
    "\n",
    "    corrected_builtup_img = prediction_image.select('predicted_label').where(prediction_image.select('predicted_label').neq(0).And(NDVI_max_img.gt(NDVI_threshold)), 0)\n",
    "    return corrected_builtup_img\n",
    "  else:\n",
    "    print(\"NDVI based builtup correction cannot be performed due to unavailability of Sentinel-2 data\")\n",
    "    return prediction_image\n",
    "\n",
    "\n",
    "def get_builtup_prediction(roi_boundary, startDate, endDate):\n",
    "  DW_ic = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .select('built','label')\n",
    "\n",
    "  builtup_img = DW_ic.select('label').mode().rename('predicted_label')\n",
    "  builtup_img = builtup_img.where(builtup_img.neq(6), 0)\n",
    "  builtup_img = builtup_img.where(builtup_img.eq(6), 1)\n",
    "\n",
    "  combined_builtup_img = builtup_img.clip(roi_boundary.geometry())\n",
    "\n",
    "  ndwi_corrected_builtup_img = ndwi_based_builtup_cleaning(roi_boundary, combined_builtup_img, startDate, endDate, 0.25)\n",
    "  ndvi_corrected_builtup_img = ndvi_based_builtup_cleaning(roi_boundary, ndwi_corrected_builtup_img, startDate, endDate, 0.5)\n",
    "\n",
    "  return ndvi_corrected_builtup_img\n",
    "\n",
    "'''\n",
    "Function to get the first date of the month of input start date and the last date of this month.\n",
    "It is used to advance the time range by 1 month in future code.\n",
    "'''\n",
    "def get_start_and_end_of_month(input_date):\n",
    "    year = input_date.get('year')\n",
    "    month = input_date.get('month')\n",
    "\n",
    "    start_of_month = ee.Date.fromYMD(year, month, 1)\n",
    "    end_of_month = start_of_month.advance(1, 'month').advance(-1, 'day')\n",
    "\n",
    "    return start_of_month, end_of_month\n",
    "\n",
    "\n",
    "'''\n",
    "Function to get water body predictions in kharif using Sentinel-1 SAR data.\n",
    "'''\n",
    "def get_kharif_bodies(roi_boundary, start_date, end_date):\n",
    "  SAR_ic = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "             .filterBounds(roi_boundary) \\\n",
    "             .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "\n",
    "  kharif_month1_ic = SAR_ic.filterDate(start_date, end_date)\n",
    "  kharif_month2_ic = SAR_ic.filterDate(start_date.advance(1, 'month'), end_date.advance(1, 'month'))\n",
    "  kharif_month3_ic = SAR_ic.filterDate(start_date.advance(2, 'month'), end_date.advance(2, 'month'))\n",
    "  kharif_month4_ic = SAR_ic.filterDate(start_date.advance(3, 'month'), end_date.advance(3, 'month'))\n",
    "\n",
    "  ###\n",
    "  ## Compute water mask\n",
    "  ###\n",
    "  kharif_month1_waterImg = kharif_month1_ic.map( lambda img: img.addBands( img.select('VV').lt(-16).rename('Water') )).select('Water').mode()\n",
    "  kharif_month2_waterImg = kharif_month2_ic.map( lambda img: img.addBands( img.select('VV').lt(-16).rename('Water') )).select('Water').mode()\n",
    "  kharif_month3_waterImg = kharif_month3_ic.map( lambda img: img.addBands( img.select('VV').lt(-16).rename('Water') )).select('Water').mode()\n",
    "  kharif_month4_waterImg = kharif_month4_ic.map( lambda img: img.addBands( img.select('VV').lt(-16).rename('Water') )).select('Water').mode()\n",
    "\n",
    "  kharif_ic = ee.ImageCollection(kharif_month1_waterImg).merge(kharif_month2_waterImg).merge(kharif_month3_waterImg).merge(kharif_month4_waterImg)\n",
    "  kharif_water_sum = kharif_ic.reduce(ee.Reducer.sum())\n",
    "  kharif_water_mask = kharif_water_sum.clip(roi_boundary.geometry()).gte(3).rename('Water')\n",
    "\n",
    "  return kharif_water_mask\n",
    "\n",
    "\n",
    "'''\n",
    "Function to get water body predictions in Rabi using Dynamic World.\n",
    "'''\n",
    "def get_rabi_bodies(roi_boundary, start_date, end_date):\n",
    "  DW_ic = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .select(['label', 'water'])\n",
    "\n",
    "  rabi_month1_ic = DW_ic.filterDate(start_date.advance(4, 'month'), end_date.advance(4, 'month'))\n",
    "  rabi_month2_ic = DW_ic.filterDate(start_date.advance(5, 'month'), end_date.advance(5, 'month'))\n",
    "  rabi_month3_ic = DW_ic.filterDate(start_date.advance(6, 'month'), end_date.advance(6, 'month'))\n",
    "  rabi_month4_ic = DW_ic.filterDate(start_date.advance(7, 'month'), end_date.advance(7, 'month'))\n",
    "\n",
    "  rabi_month1_img = ee.Image(ee.Algorithms.If( rabi_month1_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          rabi_month1_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  rabi_month2_img = ee.Image(ee.Algorithms.If( rabi_month2_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          rabi_month2_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  rabi_month3_img = ee.Image(ee.Algorithms.If( rabi_month3_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          rabi_month3_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  rabi_month4_img = ee.Image(ee.Algorithms.If( rabi_month4_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          rabi_month4_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  rabi_ic = ee.ImageCollection(rabi_month1_img).merge(rabi_month2_img).merge(rabi_month3_img).merge(rabi_month4_img)\n",
    "  rabi_water_mask = rabi_ic.reduce(ee.Reducer.sum()).gte(2).rename('Water')\n",
    "\n",
    "  return rabi_water_mask\n",
    "\n",
    "\n",
    "'''\n",
    "Function to get water body predictions in Zaid using Dynamic World.\n",
    "'''\n",
    "def get_zaid_bodies(roi_boundary, start_date, end_date):\n",
    "  DW_ic = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .select(['label', 'water'])\n",
    "\n",
    "  zaid_month1_ic = DW_ic.filterDate(start_date.advance(8, 'month'), end_date.advance(8, 'month'))\n",
    "  zaid_month2_ic = DW_ic.filterDate(start_date.advance(9, 'month'), end_date.advance(9, 'month'))\n",
    "  zaid_month3_ic = DW_ic.filterDate(start_date.advance(10, 'month'), end_date.advance(10, 'month'))\n",
    "  zaid_month4_ic = DW_ic.filterDate(start_date.advance(11, 'month'), end_date.advance(11, 'month'))\n",
    "\n",
    "  zaid_month1_img = ee.Image(ee.Algorithms.If( zaid_month1_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          zaid_month1_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  zaid_month2_img = ee.Image(ee.Algorithms.If( zaid_month2_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          zaid_month2_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  zaid_month3_img = ee.Image(ee.Algorithms.If( zaid_month3_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          zaid_month3_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  zaid_month4_img = ee.Image(ee.Algorithms.If( zaid_month4_ic.size().eq(0),\n",
    "                          ee.Image.constant(0).rename('label'),\n",
    "                          zaid_month4_ic.select('label').mode().add(1)\n",
    "                        )).clip(roi_boundary.geometry()).select('label').eq(1)\n",
    "\n",
    "  zaid_ic = ee.ImageCollection(zaid_month1_img).merge(zaid_month2_img).merge(zaid_month3_img).merge(zaid_month4_img)\n",
    "  zaid_water_mask = zaid_ic.reduce(ee.Reducer.sum()).gte(2).rename('Water')\n",
    "\n",
    "  return zaid_water_mask\n",
    "\n",
    "\n",
    "'''\n",
    "Function to clean water predictions using NDWI.\n",
    "'''\n",
    "def ndwi_based_water_cleaning(roi_boundary, prediction_image, startDate, endDate, NDWI_threshold):\n",
    "  S2_ic = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \\\n",
    "            .map(maskS2cloud) \\\n",
    "            .select(['B3', 'B8'])\n",
    "\n",
    "  if S2_ic.size().getInfo() != 0:\n",
    "    S2_ic = S2_ic.map( lambda img: img.addBands(img.normalizedDifference(['B3', 'B8']).rename('NDWI')))\n",
    "    NDWI_max_img = S2_ic.select('NDWI').max().clip(roi_boundary.geometry())\n",
    "\n",
    "    corrected_water_img = prediction_image.select('predicted_label').where(prediction_image.select('predicted_label').neq(0).And(NDWI_max_img.lt(NDWI_threshold)), 0)\n",
    "    return corrected_water_img\n",
    "  else:\n",
    "    print(\"NDWI based water correction cannot be performed due to unavailability of Sentinel-2 data\")\n",
    "    return prediction_image\n",
    "\n",
    "\n",
    "'''\n",
    "Main function to perform water classification\n",
    "'''\n",
    "def get_water_prediction(roi_boundary, startDate, endDate):\n",
    "  start_date, end_date = get_start_and_end_of_month( ee.Date(startDate) )\n",
    "\n",
    "  kharif_water_img = get_kharif_bodies(roi_boundary, start_date, end_date)\n",
    "  rabi_water_img = get_rabi_bodies(roi_boundary, start_date, end_date)\n",
    "  zaid_water_img = get_zaid_bodies(roi_boundary, start_date, end_date)\n",
    "\n",
    "  kharif_water = kharif_water_img.select('Water').rename('predicted_label')\n",
    "  rabi_water = rabi_water_img.select('Water').rename('predicted_label')\n",
    "  zaid_water = zaid_water_img.select('Water').rename('predicted_label')\n",
    "  combined_water_img = kharif_water.where(kharif_water, 2).where(rabi_water, 3).where(zaid_water, 4)\n",
    "\n",
    "  # Clean the water predictions based on confidence and NDWI\n",
    "  ndwi_corrected_img = ndwi_based_water_cleaning(roi_boundary, combined_water_img, startDate, endDate, 0.15)\n",
    "\n",
    "  return ndwi_corrected_img\n",
    "def get_barrenland_prediction(roi_boundary, startDate, endDate):\n",
    "  DW_ic = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .select('bare','label')\n",
    "\n",
    "  bare_img = DW_ic.select('label').mode().rename('predicted_label')\n",
    "  bare_img = bare_img.where(bare_img.neq(7), 0)\n",
    "\n",
    "  bare_img = bare_img.clip(roi_boundary.geometry())\n",
    "\n",
    "  return bare_img\n",
    "\n",
    "def fill_empty_bands(image):\n",
    "  band_names = image.bandNames()\n",
    "  zero_img = image.select(0).multiply(0).rename('constant').toDouble()\n",
    "  zero_img_masked = zero_img.updateMask(zero_img)\n",
    "  image = ee.Algorithms.If(ee.List(band_names).contains(ee.String('VV')),image, ee.Image(image).addBands(zero_img_masked.select('constant').rename('VV')))\n",
    "  image = ee.Algorithms.If(ee.List(band_names).contains(ee.String('VH')),image, ee.Image(image).addBands(zero_img_masked.select('constant').rename('VH')))\n",
    "  return image\n",
    "\n",
    "\n",
    "def Get_S1_ImageCollections(inputStartDate, inputEndDate, roi_boundary):\n",
    "  S1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "         .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "         .filterDate(inputStartDate, inputEndDate) \\\n",
    "         .filterBounds(roi_boundary)\n",
    "\n",
    "  S1_processed = S1.map(fill_empty_bands)\n",
    "  return S1_processed\n",
    "\n",
    "\n",
    "def GetVV_VH_image_datewise(S1_ic):\n",
    "  def get_VV_VH_datewise(date):\n",
    "    zero_img = S1_ic.first().select('VV','VH').multiply(0)\n",
    "    zero_img_masked = zero_img.updateMask(zero_img)\n",
    "\n",
    "    subset_ic = S1_ic.select(['VV','VH']).filterDate(ee.Date(date), ee.Date(date).advance(16, 'day'))\n",
    "    image = ee.Algorithms.If( ee.Number(subset_ic.size()).gt(0), subset_ic.mean().set('system:time_start',ee.Date(date).millis()), zero_img.set('system:time_start',ee.Date(date).millis()))\n",
    "\n",
    "    return image\n",
    "  return get_VV_VH_datewise\n",
    "\n",
    "\n",
    "def Get_S1_16Day_VV_VH_TimeSeries(inputStartDate, inputEndDate, S1_ic):\n",
    "  startDate = datetime.strptime(inputStartDate,\"%Y-%m-%d\")\n",
    "  endDate = datetime.strptime(inputEndDate,\"%Y-%m-%d\")\n",
    "\n",
    "  date_list = pd.date_range(start=startDate, end=endDate, freq='16D').tolist()\n",
    "  date_list = ee.List( [datetime.strftime(curr_date,\"%Y-%m-%d\") for curr_date in date_list] )\n",
    "\n",
    "  S1_TS =  ee.ImageCollection.fromImages(date_list.map(GetVV_VH_image_datewise(S1_ic)))\n",
    "  return S1_TS\n",
    "\n",
    "\n",
    "def add_sarImg_timestamp(image):\n",
    "  timeImage = image.metadata('system:time_start').rename('timestamp')\n",
    "  timeImageMasked = timeImage.updateMask(image.mask().select(0))\n",
    "  return image.addBands(timeImageMasked)\n",
    "\n",
    "\n",
    "def performInterpolation_sarTS(image):\n",
    "  image = ee.Image(image)\n",
    "  beforeImages = ee.List(image.get('before'))\n",
    "  beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "  afterImages = ee.List(image.get('after'))\n",
    "  afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "\n",
    "  # Interpolation formula\n",
    "  # y = y1 + (y2-y1)*((t – t1) / (t2 – t1))\n",
    "  # y = interpolated image\n",
    "  # y1 = before image\n",
    "  # y2 = after image\n",
    "  # t = interpolation timestamp\n",
    "  # t1 = before image timestamp\n",
    "  # t2 = after image timestamp\n",
    "\n",
    "  t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "  t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "  t = image.metadata('system:time_start').rename('t')\n",
    "  timeImage = ee.Image.cat([t1, t2, t])\n",
    "  timeRatio = timeImage.expression('(t - t1) / (t2 - t1)', {\n",
    "                  't': timeImage.select('t'),\n",
    "                  't1': timeImage.select('t1'),\n",
    "                  't2': timeImage.select('t2'),\n",
    "              })\n",
    "\n",
    "  interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "  result = image.unmask(interpolated)\n",
    "\n",
    "  #Saketh\n",
    "  #For data points on either end of time-series\n",
    "  #Before or After mosaics may still have gaps (owing to few/no images in the window)\n",
    "  #Simply fill with after mosaic (for first few data points) and before mosaic (for last few datapoints)\n",
    "  fill_value = ee.ImageCollection([beforeMosaic, afterMosaic]).mosaic()\n",
    "  result = result.unmask(fill_value)\n",
    "\n",
    "  return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "\n",
    "def interpolate_sar_timeseries(S1_TS):\n",
    "  filtered = S1_TS.map(add_sarImg_timestamp)\n",
    "\n",
    "  # Time window in which we are willing to look forward and backward for unmasked pixel in time series\n",
    "  timeWindow = 120\n",
    "\n",
    "  # Define a maxDifference filter to find all images within the specified days. Convert days to milliseconds.\n",
    "  millis = ee.Number(timeWindow).multiply(1000*60*60*24)\n",
    "  # Filter says that pick only those timestamps which lie between the 2 timestamps not more than millis difference apart\n",
    "  maxDiffFilter = ee.Filter.maxDifference(\n",
    "                              difference = millis,\n",
    "                              leftField = 'system:time_start',\n",
    "                              rightField = 'system:time_start',\n",
    "                            )\n",
    "\n",
    "  # Filter to find all images after a given image. Compare the image's timstamp against other images.\n",
    "  # Images ahead of target image should have higher timestamp.\n",
    "  lessEqFilter = ee.Filter.lessThanOrEquals(\n",
    "                            leftField = 'system:time_start',\n",
    "                            rightField = 'system:time_start'\n",
    "                          )\n",
    "\n",
    "  # Similarly define this filter to find all images before a given image\n",
    "  greaterEqFilter = ee.Filter.greaterThanOrEquals(\n",
    "                            leftField = 'system:time_start',\n",
    "                            rightField = 'system:time_start'\n",
    "                          )\n",
    "\n",
    "  # Apply first join to find all images that are after the target image but within the timeWindow\n",
    "  filter1 = ee.Filter.And( maxDiffFilter, lessEqFilter )\n",
    "  join1 = ee.Join.saveAll(\n",
    "                  matchesKey = 'after',\n",
    "                  ordering = 'system:time_start',\n",
    "                  ascending = False\n",
    "          )\n",
    "  join1Result = join1.apply(\n",
    "                  primary = filtered,\n",
    "                  secondary = filtered,\n",
    "                  condition = filter1\n",
    "                )\n",
    "\n",
    "  # Apply first join to find all images that are after the target image but within the timeWindow\n",
    "  filter2 = ee.Filter.And( maxDiffFilter, greaterEqFilter )\n",
    "  join2 = ee.Join.saveAll(\n",
    "                  matchesKey = 'before',\n",
    "                  ordering = 'system:time_start',\n",
    "                  ascending = True\n",
    "          )\n",
    "  join2Result = join2.apply(\n",
    "                  primary = join1Result,\n",
    "                  secondary = join1Result,\n",
    "                  condition = filter2\n",
    "                )\n",
    "\n",
    "  interpolated_S1_TS = ee.ImageCollection(join2Result.map(performInterpolation_sarTS))\n",
    "\n",
    "  return interpolated_S1_TS\n",
    "\n",
    "\n",
    "def get_trained_model(training_data_assetpath):\n",
    "  training_data = ee.FeatureCollection(training_data_assetpath)\n",
    "\n",
    "  training_band_names = ['0_VV', '1_VV', '2_VV', '3_VV', '4_VV', '5_VV', '6_VV', '7_VV', '8_VV', '9_VV', '10_VV', '11_VV', '12_VV', '13_VV', '14_VV', '15_VV', '16_VV', '17_VV', '18_VV', '19_VV', '20_VV', '21_VV', '22_VV',\n",
    "                '0_VH', '1_VH', '2_VH', '3_VH', '4_VH', '5_VH', '6_VH', '7_VH', '8_VH', '9_VH', '10_VH', '11_VH', '12_VH', '13_VH', '14_VH', '15_VH', '16_VH', '17_VH', '18_VH', '19_VH', '20_VH', '21_VH', '22_VH']\n",
    "\n",
    "  trained_model = ee.Classifier.smileRandomForest(numberOfTrees=100, seed=42).setOutputMode('MULTIPROBABILITY').train(\n",
    "                              features = training_data,\n",
    "                            classProperty = 'class',\n",
    "                            inputProperties = training_band_names )\n",
    "\n",
    "  return trained_model\n",
    "\n",
    "\n",
    "def Get_slope(roi_boundary):\n",
    "  dem = ee.Image('CGIAR/SRTM90_V4')\n",
    "  slope = ee.Terrain.slope(dem)\n",
    "  slope_image = slope.clip(roi_boundary.geometry())\n",
    "  return slope_image\n",
    "\n",
    "\n",
    "'''\n",
    "Function to clean cropland predictions using NDVI.\n",
    "'''\n",
    "def ndvi_based_cropland_cleaning(roi_boundary, prediction_image, startDate, endDate, NDVI_threshold):\n",
    "  S2_ic = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \\\n",
    "            .map(maskS2cloud) \\\n",
    "            .select(['B4', 'B8'])\n",
    "\n",
    "  if S2_ic.size().getInfo():\n",
    "    S2_ic = S2_ic.map( lambda img: img.addBands(img.normalizedDifference(['B8', 'B4']).rename('NDVI')))\n",
    "    NDVI_max_img = S2_ic.select('NDVI').max().clip(roi_boundary.geometry())\n",
    "\n",
    "    # Get barrenlands out as label 7\n",
    "    corrected_cropland_img = prediction_image.select('predicted_label').where(\n",
    "                              (prediction_image.select('predicted_label').eq(5))\n",
    "                                .And(NDVI_max_img.lt(NDVI_threshold)), 7)\n",
    "\n",
    "    return corrected_cropland_img\n",
    "  else:\n",
    "    print(\"NDVI based cropland correction cannot be performed due to unavailability of Sentinel-2 data\")\n",
    "    return prediction_image\n",
    "\n",
    "\n",
    "'''\n",
    "Function to clean forest/tree predictions using NDVI.\n",
    "'''\n",
    "def ndvi_based_forest_cleaning(roi_boundary, prediction_image, startDate, endDate, NDVI_threshold):\n",
    "  S2_ic = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \\\n",
    "            .map(maskS2cloud) \\\n",
    "            .select(['B4', 'B8'])\n",
    "\n",
    "  if S2_ic.size().getInfo():\n",
    "    S2_ic = S2_ic.map( lambda img: img.addBands(img.normalizedDifference(['B8', 'B4']).rename('NDVI')))\n",
    "    NDVI_max_img = S2_ic.select('NDVI').max().clip(roi_boundary.geometry())\n",
    "\n",
    "    # Get barrenlands out as label 7\n",
    "    corrected_forest_img = prediction_image.select('predicted_label').where(\n",
    "                              (prediction_image.select('predicted_label').eq(6))\n",
    "                              .And(NDVI_max_img.lt(NDVI_threshold)), 7)\n",
    "\n",
    "    return corrected_forest_img\n",
    "  else:\n",
    "    print(\"NDVI based forest correction cannot be performed due to unavailability of Sentinel-2 data\")\n",
    "    return prediction_image\n",
    "\n",
    "\n",
    "def get_cropland_prediction(startDate, endDate, roi_boundary):\n",
    "  training_data_assetpath = 'projects/ee-indiasat/assets/Rasterized_Groundtruth/L2_TrainingData_SAR_TimeSeries_1Year'\n",
    "  trained_model = get_trained_model(training_data_assetpath)\n",
    "\n",
    "  S1_ic = Get_S1_ImageCollections(startDate, endDate, roi_boundary)\n",
    "  S1_TS = Get_S1_16Day_VV_VH_TimeSeries(startDate, endDate, S1_ic)\n",
    "  interpolated_S1_TS = interpolate_sar_timeseries(S1_TS)\n",
    "  S1_TS_img = interpolated_S1_TS.toBands()\n",
    "  S1_VV_TS_img = S1_TS_img.select(['.*_VV'])\n",
    "  S1_VH_TS_img = S1_TS_img.select(['.*_VH'])\n",
    "\n",
    "  training_band_names = ['0_VV', '1_VV', '2_VV', '3_VV', '4_VV', '5_VV', '6_VV', '7_VV', '8_VV', '9_VV', '10_VV', '11_VV', '12_VV', '13_VV', '14_VV', '15_VV', '16_VV', '17_VV', '18_VV', '19_VV', '20_VV', '21_VV', '22_VV',\n",
    "                '0_VH', '1_VH', '2_VH', '3_VH', '4_VH', '5_VH', '6_VH', '7_VH', '8_VH', '9_VH', '10_VH', '11_VH', '12_VH', '13_VH', '14_VH', '15_VH', '16_VH', '17_VH', '18_VH', '19_VH', '20_VH', '21_VH', '22_VH']\n",
    "\n",
    "  training_img = S1_VV_TS_img.addBands(S1_VH_TS_img).select(training_band_names).clip(roi_boundary.geometry())\n",
    "  classified_image = training_img.classify(trained_model)\n",
    "\n",
    "  roi_label_image = classified_image.select(['classification']).arrayArgmax().arrayFlatten([['predicted_label']])\n",
    "  roi_label_image = roi_label_image.add(5).toInt8()\n",
    "\n",
    "  slope_img = Get_slope(roi_boundary)\n",
    "  combined_img = roi_label_image.addBands(slope_img)\n",
    "\n",
    "  #check if the slope is >20 deg, re-classify the pixel from cropland to non-cropland\n",
    "  final_classified_img = combined_img.select(['predicted_label']).where(\n",
    "                                              combined_img.select('predicted_label').eq(5)\n",
    "                                              .And(\n",
    "                                                    combined_img.select('slope').gte(30)\n",
    "                                            ),\n",
    "                                      6\n",
    "                                  )\n",
    "\n",
    "  cropland_corrected_img = ndvi_based_cropland_cleaning(roi_boundary, final_classified_img, startDate, endDate, NDVI_threshold=0.15)\n",
    "  forest_corrected_img = ndvi_based_forest_cleaning(roi_boundary, cropland_corrected_img, startDate, endDate, NDVI_threshold=0.3)\n",
    "\n",
    "  return forest_corrected_img\n",
    "\n",
    "def dw_based_shrub_cleaning(roi_boundary, current_prediction_output, startDate, endDate):\n",
    "    DW_ic = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') \\\n",
    "            .filterBounds(roi_boundary) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .select('shrub_and_scrub','label')\n",
    "\n",
    "    bare_img = DW_ic.select('label').mode().rename('predicted_label').clip(roi_boundary.geometry())\n",
    "    corrected_output = current_prediction_output.where(\n",
    "                              (current_prediction_output.select('predicted_label').eq(8))\n",
    "                              .And(bare_img.select('predicted_label').eq(5)), 12)\n",
    "\n",
    "    return corrected_output\n",
    "\n",
    "chastainBandNames = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2']\n",
    "\n",
    "# Regression model parameters from Table-4. MSI TOA reflectance as a function of OLI TOA reflectance.\n",
    "msiOLISlopes = [1.0946,1.0043,1.0524,0.8954,1.0049,1.0002]\n",
    "msiOLIIntercepts = [-0.0107,0.0026,-0.0015,0.0033,0.0065,0.0046]\n",
    "\n",
    "# Regression model parameters from Table-5. MSI TOA reflectance as a function of ETM+ TOA reflectance.\n",
    "msiETMSlopes = [1.10601,0.99091,1.05681,1.0045,1.03611,1.04011]\n",
    "msiETMIntercepts = [-0.0139,0.00411,-0.0024,-0.0076,0.00411,0.00861]\n",
    "\n",
    "# Regression model parameters from Table-6. OLI TOA reflectance as a function of ETM+ TOA reflectance.\n",
    "oliETMSlopes =[1.03501,1.00921,1.01991,1.14061,1.04351,1.05271];\n",
    "oliETMIntercepts = [-0.0055,-0.0008,-0.0021,-0.0163,-0.0045,0.00261]\n",
    "\n",
    "# Construct dictionary to handle all pairwise combos\n",
    "chastainCoeffDict = { 'MSI_OLI':[msiOLISlopes,msiOLIIntercepts,1], # check what the third item corresponds to\n",
    "                      'MSI_ETM':[msiETMSlopes,msiETMIntercepts,1],\n",
    "                      'OLI_ETM':[oliETMSlopes,oliETMIntercepts,1],\n",
    "                      'OLI_MSI':[msiOLISlopes,msiOLIIntercepts,0],\n",
    "                      'ETM_MSI':[msiETMSlopes,msiETMIntercepts,0],\n",
    "                      'ETM_OLI':[oliETMSlopes,oliETMIntercepts,0]\n",
    "                    }\n",
    "\n",
    "\n",
    "'''\n",
    "Function to mask cloudy pixels in Landsat-7\n",
    "'''\n",
    "def maskL7cloud(image):\n",
    "  qa = image.select('QA_PIXEL')\n",
    "  mask = qa.bitwiseAnd(1 << 4).eq(0)\n",
    "  return image.updateMask(mask).select(['B1', 'B2', 'B3' , 'B4' , 'B5' , 'B7']).rename('BLUE', 'GREEN', 'RED' , 'NIR' , 'SWIR1' , 'SWIR2')\n",
    "\n",
    "\n",
    "'''\n",
    "Function to mask cloudy pixels in Landsat-8\n",
    "'''\n",
    "def maskL8cloud(image):\n",
    "  qa = image.select('QA_PIXEL')\n",
    "  mask = qa.bitwiseAnd(1 << 4).eq(0)\n",
    "  return image.updateMask(mask).select(['B2', 'B3', 'B4' , 'B5' , 'B6' , 'B7']).rename('BLUE', 'GREEN', 'RED' , 'NIR' , 'SWIR1' , 'SWIR2')\n",
    "\n",
    "\n",
    "'''\n",
    "Function to mask clouds using the quality band of Sentinel-2 TOA\n",
    "'''\n",
    "def maskS2cloudTOA(image):\n",
    "  qa = image.select('QA60')\n",
    "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "  cloudBitMask = 1 << 10\n",
    "  cirrusBitMask = 1 << 11\n",
    "  # Both flags should be set to zero, indicating clear conditions.\n",
    "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0));\n",
    "  return image.updateMask(mask).select(['B2', 'B3', 'B4', 'B8',  'B11', 'B12']).rename(['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2'])\n",
    "\n",
    "\n",
    "'''\n",
    "Get Landsat and Sentinel image collections\n",
    "'''\n",
    "def Get_L7_L8_S2_ImageCollections(inputStartDate, inputEndDate, roi_boundary):\n",
    "  # ------ Landsat 7 TOA\n",
    "  L7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_TOA') \\\n",
    "          .filterDate(inputStartDate, inputEndDate) \\\n",
    "          .filterBounds(roi_boundary) \\\n",
    "          .map(maskL7cloud)\n",
    "  # print('\\n Original Landsat 7 TOA dataset: \\n',L7.limit(1).getInfo())\n",
    "  # print('Number of images in Landsat 7 TOA dataset: \\t',L7.size().getInfo())\n",
    "\n",
    "  # ------ Landsat 8 TOA\n",
    "  L8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_TOA') \\\n",
    "          .filterDate(inputStartDate, inputEndDate) \\\n",
    "          .filterBounds(roi_boundary) \\\n",
    "          .map(maskL8cloud)\n",
    "  # print('\\n Original Landsat 8 TOA dataset: \\n', L8.limit(1).getInfo())\n",
    "  # print('Number of images in Landsat 8 TOA dataset: \\t',L8.size().getInfo())\n",
    "\n",
    "  # ------ Sentinel-2 TOA\n",
    "  S2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
    "          .filterDate(inputStartDate, inputEndDate) \\\n",
    "          .filterBounds(roi_boundary)  \\\n",
    "          .map(maskS2cloudTOA)\n",
    "  # print('\\n Original Sentinel-2 TOA dataset: \\n',S2.limit(1).getInfo())\n",
    "  # print('Number of images in Sentinel 2 TOA dataset: \\t',S2.size().getInfo())\n",
    "\n",
    "  return L7, L8, S2\n",
    "\n",
    "\n",
    "'''\n",
    "Function to apply model in one direction\n",
    "'''\n",
    "def dir0Regression(img,slopes,intercepts):\n",
    "  return img.select(chastainBandNames).multiply(slopes).add(intercepts)\n",
    "\n",
    "\n",
    "'''\n",
    "Applying the model in the opposite direction\n",
    "'''\n",
    "def dir1Regression(img,slopes,intercepts):\n",
    "  return img.select(chastainBandNames).subtract(intercepts).divide(slopes)\n",
    "\n",
    "\n",
    "'''\n",
    "Function to correct one sensor to another\n",
    "'''\n",
    "def harmonizationChastain(img, fromSensor,toSensor):\n",
    "  # Get the model for the given from and to sensor\n",
    "  comboKey = fromSensor.upper() + '_' + toSensor.upper()\n",
    "  coeffList = chastainCoeffDict[comboKey]\n",
    "  slopes = coeffList[0]\n",
    "  intercepts = coeffList[1]\n",
    "  direction = ee.Number(coeffList[2])\n",
    "\n",
    "  # Apply the model in the respective direction\n",
    "  out = ee.Algorithms.If(direction.eq(0),dir0Regression(img,slopes,intercepts),dir1Regression(img,slopes,intercepts))\n",
    "  return ee.Image(out).copyProperties(img).copyProperties(img,['system:time_start'])\n",
    "\n",
    "\n",
    "'''\n",
    "Calibrate Landsat-8 (OLI) and Sentinel-2 (MSI) to Landsat-7 (ETM+)\n",
    "'''\n",
    "def Harmonize_L7_L8_S2(L7, L8, S2):\n",
    "  # harmonization\n",
    "  harmonized_L8 = L8.map( lambda img: harmonizationChastain(img, 'OLI','ETM') )\n",
    "  harmonized_S2 = S2.map( lambda img: harmonizationChastain(img, 'MSI','ETM') )\n",
    "\n",
    "  # Merge harmonized landsat-8 and sentinel-2 to landsat-7 image collection\n",
    "  harmonized_LandsatSentinel_ic = ee.ImageCollection(L7.merge(harmonized_L8).merge(harmonized_S2))\n",
    "  # print(harmonized_LandsatSentinel_ic.size().getInfo())\n",
    "  return harmonized_LandsatSentinel_ic\n",
    "\n",
    "\n",
    "'''\n",
    "Add NDVI band to harmonized image collection\n",
    "'''\n",
    "def addNDVI(image):\n",
    "  return image.addBands(image.normalizedDifference(['NIR', 'RED']).rename('NDVI')).float()\n",
    "\n",
    "\n",
    "'''\n",
    "Function definitions to get NDVI values at each 16-day composites\n",
    "'''\n",
    "def Get_NDVI_image_datewise(harmonized_LS_ic, roi_boundary):\n",
    "  def get_NDVI_datewise(date):\n",
    "    empty_band_image = ee.Image(0).float().rename(['NDVI']).updateMask(ee.Image(0).clip(roi_boundary))\n",
    "    return harmonized_LS_ic.select(['NDVI']) \\\n",
    "                            .filterDate(ee.Date(date), ee.Date(date).advance(16, 'day')) \\\n",
    "                            .merge(empty_band_image)\\\n",
    "                            .median() \\\n",
    "                            .set('system:time_start',ee.Date(date).millis())\n",
    "  return get_NDVI_datewise\n",
    "\n",
    "def Get_LS_16Day_NDVI_TimeSeries(inputStartDate, inputEndDate, harmonized_LS_ic, roi_boundary):\n",
    "  startDate = datetime.strptime(inputStartDate,\"%Y-%m-%d\")\n",
    "  endDate = datetime.strptime(inputEndDate,\"%Y-%m-%d\")\n",
    "\n",
    "  date_list = pd.date_range(start=startDate, end=endDate, freq='16D').tolist()\n",
    "  date_list = ee.List( [datetime.strftime(curr_date,\"%Y-%m-%d\") for curr_date in date_list] )\n",
    "\n",
    "  LSC =  ee.ImageCollection.fromImages(date_list.map(Get_NDVI_image_datewise(harmonized_LS_ic, roi_boundary)))\n",
    "\n",
    "  return LSC\n",
    "\n",
    "\n",
    "'''\n",
    "Pair available LSC and modis values for each time stamp.\n",
    "'''\n",
    "def pairLSModis(lsRenameBands):\n",
    "  def pair(feature):\n",
    "    date = ee.Date( feature.get('system:time_start') )\n",
    "    startDateT = date.advance(-8,'day')\n",
    "    endDateT = date.advance(8,'day')\n",
    "\n",
    "    # ------ MODIS VI ( We can add EVI to the band list later )\n",
    "    modis = ee.ImageCollection('MODIS/061/MOD13Q1') \\\n",
    "              .filterDate(startDateT, endDateT) \\\n",
    "              .select(['NDVI','SummaryQA']) \\\n",
    "              .filterBounds(roi_boundary) \\\n",
    "              .median() \\\n",
    "              .rename(['NDVI_modis', 'SummaryQA_modis'])\n",
    "\n",
    "    return feature.rename(lsRenameBands).addBands(modis)\n",
    "  return pair\n",
    "\n",
    "\n",
    "'''\n",
    "Function to get Pearson Correlation Coffecient to perform GapFilling\n",
    "'''\n",
    "def get_Pearson_Correlation_Coefficients(LSC_modis_paired_ic, roi_boundary, bandList):\n",
    "  corr = LSC_modis_paired_ic.filterBounds(roi_boundary) \\\n",
    "                            .select(bandList).toArray() \\\n",
    "                            .arrayReduce( reducer = ee.Reducer.pearsonsCorrelation(), axes=[0], fieldAxis=1 ) \\\n",
    "                            .arrayProject([1]).arrayFlatten([['c', 'p']])\n",
    "  return corr\n",
    "\n",
    "\n",
    "'''Use print(...) to write to this console.\n",
    "Fill gaps in LSC timeseries using modis data\n",
    "'''\n",
    "def gapfillLSM(LSC_modis_regression_model, LSC_bandName, modis_bandName):\n",
    "  def peformGapfilling(image):\n",
    "    offset = LSC_modis_regression_model.select('offset')\n",
    "    scale = LSC_modis_regression_model.select('scale')\n",
    "    nodata = -1\n",
    "\n",
    "    lsc_image = image.select(LSC_bandName)\n",
    "    modisfit = image.select(modis_bandName).multiply(scale).add(offset)\n",
    "\n",
    "    mask = lsc_image.mask()#update mask needs an input (no default input from the API document)\n",
    "    gapfill = lsc_image.unmask(nodata)\n",
    "    gapfill = gapfill.where(mask.Not(), modisfit)\n",
    "\n",
    "    '''\n",
    "    in SummaryQA,\n",
    "    0: Good data, use with confidence\n",
    "    1: Marginal data, useful but look at detailed QA for more information\n",
    "    2: Pixel covered with snow/ice\n",
    "    3: Pixel is cloudy\n",
    "    '''\n",
    "    qc_m = image.select('SummaryQA_modis').unmask(3)  # missing value is grouped as cloud\n",
    "    w_m  = modisfit.mask().rename('w_m').where(qc_m.eq(0), 0.8)  # default is 0.8\n",
    "    w_m = w_m.where(qc_m.eq(1), 0.5)   # Marginal\n",
    "    w_m = w_m.where(qc_m.gte(2), 0.2) # snow/ice or cloudy\n",
    "\n",
    "    # make sure these modis values are read where there is missing data from LandSat, Sentinel\n",
    "    w_l = gapfill.mask() # default is 1\n",
    "    w_l = w_l.where(mask.Not(), w_m)\n",
    "\n",
    "    return gapfill.addBands(w_l).rename(['gapfilled_'+LSC_bandName,'SummaryQA']) #have NDVI from modis and a summary of clarity for each\n",
    "\n",
    "  return peformGapfilling\n",
    "\n",
    "\n",
    "'''\n",
    "Function to combine LSC with Modis data\n",
    "'''\n",
    "def Combine_LS_Modis(LSC):\n",
    "  lsRenameBands = ee.Image(LSC.first()).bandNames().map( lambda band: ee.String(band).cat('_lsc') )\n",
    "  LSC_modis_paired_ic = LSC.map( pairLSModis(lsRenameBands) )\n",
    "\n",
    "  # Output contains scale, offset i.e. two bands\n",
    "  LSC_modis_regression_model_NDVI = LSC_modis_paired_ic.select(['NDVI_modis', 'NDVI_lsc']) \\\n",
    "                                                        .reduce(ee.Reducer.linearFit())\n",
    "\n",
    "  corr_NDVI = get_Pearson_Correlation_Coefficients(LSC_modis_paired_ic, roi_boundary, ['NDVI_modis', 'NDVI_lsc'])\n",
    "  LSMC_NDVI = LSC_modis_paired_ic.map(gapfillLSM(LSC_modis_regression_model_NDVI, 'NDVI_lsc', 'NDVI_modis'))\n",
    "\n",
    "  return LSMC_NDVI\n",
    "\n",
    "\n",
    "'''\n",
    "Mask out low quality pixels\n",
    "'''\n",
    "def mask_low_QA(lsmc_image):\n",
    "  low_qa = lsmc_image.select('SummaryQA').neq(0.2)\n",
    "  return lsmc_image.updateMask(low_qa).copyProperties(lsmc_image, ['system:time_start'])\n",
    "\n",
    "\n",
    "'''\n",
    "Add image timestamp to each image in time series\n",
    "'''\n",
    "def add_timestamp(image):\n",
    "  timeImage = image.metadata('system:time_start').rename('timestamp')\n",
    "  timeImageMasked = timeImage.updateMask(image.mask().select(0))\n",
    "  return image.addBands(timeImageMasked)\n",
    "\n",
    "\n",
    "'''\n",
    "Perform linear interpolation on missing values\n",
    "'''\n",
    "def performInterpolation(image):\n",
    "  image = ee.Image(image)\n",
    "  beforeImages = ee.List(image.get('before'))\n",
    "  beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "  afterImages = ee.List(image.get('after'))\n",
    "  afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "\n",
    "  # Interpolation formula\n",
    "  # y = y1 + (y2-y1)*((t – t1) / (t2 – t1))\n",
    "  # y = interpolated image\n",
    "  # y1 = before image\n",
    "  # y2 = after image\n",
    "  # t = interpolation timestamp\n",
    "  # t1 = before image timestamp\n",
    "  # t2 = after image timestamp\n",
    "\n",
    "  t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "  t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "  t = image.metadata('system:time_start').rename('t')\n",
    "  timeImage = ee.Image.cat([t1, t2, t])\n",
    "  timeRatio = timeImage.expression('(t - t1) / (t2 - t1)', {\n",
    "                  't': timeImage.select('t'),\n",
    "                  't1': timeImage.select('t1'),\n",
    "                  't2': timeImage.select('t2'),\n",
    "              })\n",
    "\n",
    "  interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "  result = image.unmask(interpolated)\n",
    "  fill_value = ee.ImageCollection([beforeMosaic, afterMosaic]).mosaic()\n",
    "  result = result.unmask(fill_value)\n",
    "\n",
    "  return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "\n",
    "def interpolate_timeseries(S1_TS):\n",
    "  lsmc_masked = S1_TS.map(mask_low_QA)\n",
    "  filtered = lsmc_masked.map(add_timestamp)\n",
    "\n",
    "  # Time window in which we are willing to look forward and backward for unmasked pixel in time series\n",
    "  timeWindow = 120\n",
    "\n",
    "  # Define a maxDifference filter to find all images within the specified days. Convert days to milliseconds.\n",
    "  millis = ee.Number(timeWindow).multiply(1000*60*60*24)\n",
    "  # Filter says that pick only those timestamps which lie between the 2 timestamps not more than millis difference apart\n",
    "  maxDiffFilter = ee.Filter.maxDifference(\n",
    "                              difference = millis,\n",
    "                              leftField = 'system:time_start',\n",
    "                              rightField = 'system:time_start',\n",
    "                            )\n",
    "\n",
    "  # Filter to find all images after a given image. Compare the image's timstamp against other images.\n",
    "  # Images ahead of target image should have higher timestamp.\n",
    "  lessEqFilter = ee.Filter.lessThanOrEquals(\n",
    "                            leftField = 'system:time_start',\n",
    "                            rightField = 'system:time_start'\n",
    "                          )\n",
    "\n",
    "  # Similarly define this filter to find all images before a given image\n",
    "  greaterEqFilter = ee.Filter.greaterThanOrEquals(\n",
    "                            leftField = 'system:time_start',\n",
    "                            rightField = 'system:time_start'\n",
    "                          )\n",
    "\n",
    "  # Apply first join to find all images that are after the target image but within the timeWindow\n",
    "  filter1 = ee.Filter.And( maxDiffFilter, lessEqFilter )\n",
    "  join1 = ee.Join.saveAll(\n",
    "                  matchesKey = 'after',\n",
    "                  ordering = 'system:time_start',\n",
    "                  ascending = False\n",
    "          )\n",
    "  join1Result = join1.apply(\n",
    "                  primary = filtered,\n",
    "                  secondary = filtered,\n",
    "                  condition = filter1\n",
    "                )\n",
    "\n",
    "  # Apply first join to find all images that are after the target image but within the timeWindow\n",
    "  filter2 = ee.Filter.And( maxDiffFilter, greaterEqFilter )\n",
    "  join2 = ee.Join.saveAll(\n",
    "                  matchesKey = 'before',\n",
    "                  ordering = 'system:time_start',\n",
    "                  ascending = True\n",
    "          )\n",
    "  join2Result = join2.apply(\n",
    "                  primary = join1Result,\n",
    "                  secondary = join1Result,\n",
    "                  condition = filter2\n",
    "                )\n",
    "\n",
    "  interpolated_S1_TS = ee.ImageCollection(join2Result.map(performInterpolation))\n",
    "\n",
    "  return interpolated_S1_TS\n",
    "\n",
    "\n",
    "'''\n",
    "Function Definition to get Padded NDVI LSMC timeseries image for a given ROI\n",
    "'''\n",
    "def Get_Padded_NDVI_TS_Image(startDate, endDate, roi_boundary):\n",
    "  L7, L8, S2 = Get_L7_L8_S2_ImageCollections(startDate, endDate, roi_boundary)\n",
    "\n",
    "  harmonized_LS_ic = Harmonize_L7_L8_S2(L7, L8, S2)\n",
    "  harmonized_LS_ic = harmonized_LS_ic.map(addNDVI)\n",
    "  LSC = Get_LS_16Day_NDVI_TimeSeries(startDate, endDate, harmonized_LS_ic, roi_boundary)\n",
    "  LSMC_NDVI = Combine_LS_Modis(LSC)\n",
    "  Interpolated_LSMC_NDVI = interpolate_timeseries(LSMC_NDVI)\n",
    "  final_LSMC_NDVI_TS = Interpolated_LSMC_NDVI.select(['gapfilled_NDVI_lsc']).toBands()\n",
    "  final_LSMC_NDVI_TS = final_LSMC_NDVI_TS.clip(roi_boundary)\n",
    "\n",
    "  input_bands = final_LSMC_NDVI_TS.bandNames()\n",
    "  return final_LSMC_NDVI_TS, input_bands\n",
    "\n",
    "\n",
    "'''\n",
    "Function definition to compute euclidean distance to each cluster centroid\n",
    "features ---> clusters\n",
    "flattened ---> time series image clipped to target area\n",
    "input_bands ---> band names for time series image\n",
    "studyarea ---> geometry of region of interest\n",
    "'''\n",
    "# Function to get distances as required from each pixel to each cluster centroid\n",
    "def Get_Euclidean_Distance(cluster_centroids, roi_timeseries_img, input_bands, roi_boundary):\n",
    "\n",
    "  def wrapper(curr_centroid):\n",
    "    temp_img = ee.Image()\n",
    "    curr_centroid = ee.Feature(curr_centroid).setGeometry(roi_boundary)\n",
    "    temp_fc = ee.FeatureCollection( [curr_centroid] )\n",
    "    class_img = temp_fc.select(['class']).reduceToImage(['class'], ee.Reducer.first()).rename(['class'])\n",
    "    def create_img(band_name):\n",
    "      return temp_fc.select([band_name]).reduceToImage([band_name], ee.Reducer.first()).rename([band_name])\n",
    "\n",
    "    temp_img = input_bands.map(create_img)\n",
    "    empty = ee.Image()\n",
    "    temp_img = ee.Image( temp_img.iterate( lambda img, prev: ee.Image(prev).addBands(img) , empty))\n",
    "\n",
    "    temp_img = temp_img.select(temp_img.bandNames().remove('constant'))\n",
    "    distance = roi_timeseries_img.spectralDistance(temp_img, 'sed')\n",
    "    confidence = ee.Image(1.0).divide(distance).rename(['confidence'])\n",
    "    distance = distance.addBands(confidence)\n",
    "    return distance.addBands(class_img.rename(['class'])).set('class', curr_centroid.get('class'))\n",
    "\n",
    "  return cluster_centroids.map(wrapper)\n",
    "\n",
    "\n",
    "'''\n",
    "Function definition to get final prediction image from distance images\n",
    "'''\n",
    "def Get_final_prediction_image(distance_imgs_list):\n",
    "  # Denominator is an image that is sum of all confidences to each cluster\n",
    "  sum_of_distances = ee.ImageCollection( distance_imgs_list ).select(['confidence']).sum().unmask()\n",
    "  distance_imgs_ic = ee.ImageCollection( distance_imgs_list ).select(['distance','class'])\n",
    "\n",
    "  # array is an image where distance band is an array of distances to each cluster centroid and class band is an array of classes associated with each cluster\n",
    "  array_img = ee.ImageCollection(distance_imgs_ic).toArray()\n",
    "\n",
    "  axes = {'image': 0, 'band':1}\n",
    "  sort = array_img.arraySlice(axes['band'], 0, 1)\n",
    "  sorted = array_img.arraySort(sort)\n",
    "\n",
    "  # take the first image only\n",
    "  values = sorted.arraySlice(axes['image'], 0, 1)\n",
    "  # convert back to an image\n",
    "  min = values.arrayProject([axes['band']]).arrayFlatten([['distance', 'class']])\n",
    "  # Extract the hard classification\n",
    "  predicted_class_img = min.select(1)\n",
    "  predicted_class_img = predicted_class_img.rename(['predicted_label'])\n",
    "\n",
    "  return predicted_class_img\n",
    "\n",
    "## My Helper Functions\n",
    "def change_clusters(cluster_centroids):\n",
    "  size = cluster_centroids.size().getInfo()\n",
    "  features = []\n",
    "  for i in range(size):\n",
    "      features.append(ee.Feature(cluster_centroids.toList(size).get(i)).set(\"class\", 13+i))\n",
    "  return ee.FeatureCollection(features)\n",
    "\n",
    "\n",
    "def get_cropping_frequency(roi_boundary, startDate, endDate):\n",
    "  cluster_centroids = ee.FeatureCollection('projects/ee-indiasat/assets/L3_LULC_Clusters/Final_Level3_PanIndia_Clusters')\n",
    "  ignore_clusters = [12] # remove invalid clusters\n",
    "  cluster_centroids = cluster_centroids.filter(ee.Filter.Not( ee.Filter.inList('class', ignore_clusters)))\n",
    "  \n",
    "  final_LSMC_NDVI_TS, input_bands =  Get_Padded_NDVI_TS_Image(startDate, endDate, roi_boundary)\n",
    "  distance_imgs_list = Get_Euclidean_Distance(cluster_centroids, final_LSMC_NDVI_TS, input_bands, roi_boundary)\n",
    "  final_classified_img = Get_final_prediction_image(distance_imgs_list)\n",
    "  ### adding Cluster values after 12\n",
    "  #cluster_centroids = change_clusters(cluster_centroids)\n",
    "  distance_imgs_list = Get_Euclidean_Distance(cluster_centroids, final_LSMC_NDVI_TS, input_bands, roi_boundary)\n",
    "  final_cluster_classified_img = Get_final_prediction_image(distance_imgs_list)\n",
    "  final_cluster_classified_img = final_cluster_classified_img.rename(['predicted_cluster'])\n",
    "  final_classified_img = final_classified_img.addBands(final_cluster_classified_img)\n",
    "  return final_classified_img, final_LSMC_NDVI_TS\n",
    "\n",
    "\n",
    "roi_boundary = ee.FeatureCollection(\"users/mtpictd/agro_eco_regions\").filter(ee.Filter.eq(\"ae_regcode\", AEZ_no))\n",
    "filename_prefix = \"AEZ_\" + str(AEZ_no)\n",
    "\n",
    "mapping = {\n",
    "    \"farm\": 1,\n",
    "    \"plantation\": 2,\n",
    "    \"scrubland\": 3,\n",
    "    \"rest\": 0\n",
    "}\n",
    "\n",
    "emb = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n",
    "\n",
    "emb = emb.filterDate('2024-01-01', '2025-01-01').filterBounds(roi_boundary).mosaic()\n",
    "\n",
    "#all_samples = emb.sampleRegions(\n",
    "#  collection=samples,\n",
    "#  scale=10,\n",
    "#  geometries=False\n",
    "#)\n",
    "\n",
    "#write_to_gee(all_samples, 'projects/raman-461708/assets/AEZ_' +str(AEZ_no) + '_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d5a2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_emb_0\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_emb_0 completed successfully.\n",
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_emb_1\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_emb_1 completed successfully.\n",
      "Started export to GEE asset: projects/raman-461708/assets/AEZ_6_samples_emb_2\n",
      "Export status: READY\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: RUNNING\n",
      "Export status: COMPLETED\n",
      "Export to GEE asset projects/raman-461708/assets/AEZ_6_samples_emb_2 completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def export_samples_in_chunks(image, samples, chunk_size, asset_prefix):\n",
    "    total = samples.size().getInfo()\n",
    "    assets = []\n",
    "    for index, i in enumerate(range(0, total, chunk_size)):\n",
    "        # Get a chunk of features\n",
    "        chunk = ee.FeatureCollection(samples.toList(chunk_size, i))\n",
    "        # Sample the image at these points\n",
    "        chunk_samples = image.sampleRegions(\n",
    "            collection=chunk,\n",
    "            scale=10,\n",
    "            geometries=True\n",
    "        )\n",
    "        # Export this chunk\n",
    "        asset_id = f\"{asset_prefix}_emb_{index}\"\n",
    "        write_to_gee(chunk_samples, asset_id)\n",
    "        assets.append(asset_id)\n",
    "    return assets\n",
    "\n",
    "sample_assets = export_samples_in_chunks(\n",
    "    emb,  # your image\n",
    "    samples,  # your FeatureCollection\n",
    "    chunk_size=30000,  # adjust as needed (try 1000-10000)\n",
    "    asset_prefix=f'projects/raman-461708/assets/AEZ_{AEZ_no}_samples'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ca74196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXECUTING LULC PREDICTION FOR  2023-07-01  TO  2024-06-30 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read each asset path as a FeatureCollection and merge them\n",
    "all_samples = ee.FeatureCollection([])  # start with empty\n",
    "for asset_path in sample_assets:\n",
    "    fc = ee.FeatureCollection(asset_path)\n",
    "    all_samples = all_samples.merge(fc)\n",
    "\n",
    "def get_classifier(bandnames):\n",
    "    classifier = ee.Classifier.smileRandomForest(numberOfTrees=100, seed=42).train(\n",
    "        features=all_samples,\n",
    "        classProperty='label',\n",
    "        inputProperties=bandnames\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "def get_emb_date(date):\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    next_year = date.year + 1\n",
    "    return f\"{next_year}-01-01\"\n",
    "\n",
    "startDate = '2023-07-01'\n",
    "endDate = '2024-07-01'\n",
    "\n",
    "L1_asset_new = []\n",
    "final_output_filename_array_new = []\n",
    "final_output_assetid_array_new = []\n",
    "crop_freq_array = []\n",
    "\n",
    "scale = 10\n",
    "\n",
    "loopStart = startDate\n",
    "loopEnd = (datetime.strptime(endDate,\"%Y-%m-%d\")).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "refined_year = False\n",
    "inde = 0\n",
    "while loopStart != loopEnd:\n",
    "    currStartDate = datetime.strptime(loopStart,\"%Y-%m-%d\")\n",
    "    currEndDate = (currStartDate+relativedelta(years=1)-timedelta(days=1))\n",
    "    loopStart = (currStartDate+relativedelta(years=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    currStartDate = currStartDate.strftime(\"%Y-%m-%d\")\n",
    "    currEndDate = currEndDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(\"\\n EXECUTING LULC PREDICTION FOR \",currStartDate,\" TO \",currEndDate,\"\\n\")\n",
    "\n",
    "    curr_filename = filename_prefix + '_' + currStartDate + \"_\" + currEndDate\n",
    "\n",
    "    if datetime.strptime(currStartDate,\"%Y-%m-%d\").year < 2017:\n",
    "        print(\"To generate LULC output of year \",datetime.strptime(currStartDate,\"%Y-%m-%d\").year,\" , go to cell-LULC execution for years before 2017\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # LULC prediction code\n",
    "    bu_image = get_builtup_prediction(roi_boundary, currStartDate, currEndDate)\n",
    "    water_image = get_water_prediction(roi_boundary, currStartDate, currEndDate)\n",
    "    combined_water_builtup_img = bu_image.where(bu_image.select('predicted_label').eq(0), water_image)\n",
    "    bare_image = get_barrenland_prediction(roi_boundary, currStartDate, currEndDate)\n",
    "    combined_water_builtup_barren_img = combined_water_builtup_img.where(combined_water_builtup_img.select('predicted_label').eq(0), bare_image)\n",
    "    \n",
    "    cropping_frequency_img, time_series_data = get_cropping_frequency(roi_boundary, currStartDate, currEndDate)\n",
    "\n",
    "    embStartDate = get_emb_date(currStartDate)\n",
    "    embEndDate = get_emb_date(currEndDate)\n",
    "    embeddings = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\").filterDate(embStartDate, embEndDate).filterBounds(roi_boundary).mosaic()\n",
    "    classifier = get_classifier(embeddings.bandNames())\n",
    "\n",
    "    scrubland_farm_raster = embeddings.classify(classifier).rename('predicted_label')\n",
    "    scrubland_farm_raster = scrubland_farm_raster.where(scrubland_farm_raster.eq(mapping[\"farm\"]), 5).where(scrubland_farm_raster.eq(mapping[\"scrubland\"]), 12).where(scrubland_farm_raster.eq(mapping[\"plantation\"]), 13)\n",
    "    combined_img = combined_water_builtup_barren_img.where(combined_water_builtup_barren_img.select('predicted_label').eq(0), scrubland_farm_raster)\n",
    "    \n",
    "    cropland_image = get_cropland_prediction(currStartDate, currEndDate, roi_boundary)\n",
    "    tree_image = cropland_image.where(cropland_image.select('predicted_label').eq(5), 12)\n",
    "    combined_img = combined_img.where(combined_img.select('predicted_label').eq(12), tree_image)\n",
    "    final_lulc_img = combined_img.addBands(ee.Image.constant(-1).rename(['predicted_cluster'])).where(combined_img.select('predicted_label').eq(5), cropping_frequency_img)\n",
    "\n",
    "    final_output_filename = curr_filename+'_LULCmap_'+str(scale)+'m'\n",
    "    final_output_assetid = 'projects/ee-raman/assets/LULC_Version2_Outputs_NewHierarchy/'+final_output_filename\n",
    "\n",
    "    final_output_filename_array_new.append(final_output_filename)\n",
    "    final_output_assetid_array_new.append(final_output_assetid)\n",
    "    L1_asset_new.append(final_lulc_img)\n",
    "    # displayMap(roi_boundary, final_lulc_img.select('predicted_label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "092a0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ee.batch.Export.image.toAsset(\n",
    "    image=final_lulc_img,#.select(\"predicted_label\"),\n",
    "    description='lulc_' + filename_prefix + \"_v4\",\n",
    "    assetId='projects/raman-461708/assets/'+filename_prefix + \"_v4\",\n",
    "    pyramidingPolicy = {'predicted_label': 'mode'},\n",
    "    scale = 10,\n",
    "    maxPixels = 1e13,\n",
    "    crs = 'EPSG:4326'\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b197b66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.size().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a93704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
